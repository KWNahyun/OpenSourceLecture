애융경진대회 발표 대본

[1] 안녕하세요, 저희는 팀 '팀장이감자'입니다. 팀원은 ai융합학부 23학변 류승희, 권나현, 문재민입니다. 저희는 컴쟁이들을 위한 자세 잔소리 프로그램 제안이라는 주제로 발표를 준비하였습니다.

[2] 먼저 목차를 소개하겠습니다. 첫번째 목표입니다. 여기에서는 저희가 최종적으로 구현하고 싶은 프로그램의 모습에 대해 소개할 것입니다. 두번째 배경에서는 연구 배경, 기술 소개, 기존 연구 사례에 대해 말씀드리겠습니다. 마지막으로 컨텐츠입니다. 이 파트에서는 문제 정의, 플로우 차트, 기술 스택을 설명하고 테스트 시연 모습을 보여드리겠습니다.

[3] 저희가 목표하고 있는 프로그램의 모습입니다. 스마트폰 카메라로부터 실시간 영상을 입력받아 분석하고 웹을 통해 사용자에게 보여줍니다. 분석한 결과는 점수로 계산해 사용자에게 보여주어 사용자가 현재 자신의 자세를 판단하는 것을 도와줍니다. 사용자의 자세가 올바르지 않은 상태가 일정 시간 이상 유지되면 문구나 음성으로 경고하여 사용자가 스스로 자세를 고칠 수 있도록 합니다.

[4] 다음으로 연구 배경입니다. 현대 사회에서 전자기기 사용 증가로 인해 척추 질환을 앓고 있는 사람들이 급격하게 늘고 있습니다. 거북목과 같은 질환은 다양한 합병증을 야기할 수 있으며 심해질 경우 일상생활까지 망가트릴 수 있습니다. 장시간동안 모니터를 보며 키보드를 두드리는 작업을 할 때 생기는 각종 신체적 정신적 장애를 통틀어 VDT 증후군이라고 합니다. 현대 사람들은 직업, 학업등의 이유로 컴퓨터 사용이 필수적인 경우가 많습니다. 이때 앉아서 컴퓨터를 하는 자세를 효과적으로 교정할 수 있다면 몸에 버그가 생기는 현상을 완화할 수 있을 것이라 생각하고 이러한 기획을 하였습니다.

[5] 이 프로그램을 구현하는 데에 필요한 핵심 기술에 대해 소개하겠습니다. 사용자의 모습인 시각 자료를 입력받아 분석해야 하므로 컴퓨터 비전 기술이 필요합니다. 컴퓨터 비전은 인공지능의 한 분야로 디지털 이미지, 비디오 및 기타 시각적 입력에서 의미 있는 정보를 추출한 다음 이러한 정보를 바탕으로 작업을 실행할 수 있도록 합니다. 컴퓨터 비전은 자율 주행, x-ray 분석, 생산라인에서 누락된 상품 탐지, 이미지 생성 등 다양한 분야에서 적극적으로 활용되고 있는 인공지능 기술입니다. 
또한, 입력받은 사용자의 모습을 분석할 때 자세 추정 기술이 활용됩니다. 자세 추정은 컴퓨터 비전을 활용한 기술 중 하나로 ML 모델을 사용하여 주요 신체 관절의 공간적 위치를 추정하여 이미지 또는 비디오로부터 사람의 포즈를 추정하는 작업입니다. 얼굴 표정을 예측하거나, 스포츠 경기에서 선수들의 움직임을 분석하는 데에 사용될 수 있고, 플레이어의 제스처를 인식하는 게임에서 활용할 수 있습니다.

[6] 기존에 머신러닝을 이용해 거북목을 진단하는 모델에 대한 연구들이 있습니다. 많은 연구들이 Openpose 오픈소스와 COCO 오픈소스 데이터셋을 이용하여 합성곱 신경망 모델을 만드는 것을 볼 수 있었습니다. 기존의 연구들은 주로 거북목만을 진단하는 연구들이 대다수였습니다. 또한, 연구들이 실제로 쉽게 사용될 수 있는 어플로 만들어진 사례는 없었습니다. 저희는 이러한 점들을 보완할 예정입니다.

[7] 다음으로 저희가 정의한 문제에 대해 설명해드리겠습니다. 컴퓨터 사용자들은 오랜 시간 컴퓨터를 사용하다 보면, 잘못된 자세로 작업할 때가 많습니다. 하지만 제 3자가 지적해 주기 전까지는 스스로 인지하기 어려워 자세를 고치기 힘들다고 생각하였습니다. 이를 극복하기 위해 사용자가 컴퓨터를 하고 있는 측면의 모습을 실시간으로 분석하고 사용자에게 보여주며 바르지 않은 자세에 대해 경고도 해주는 프로그램을 기획하였습다. 프로그램으로 제 3자의 눈을 구현하는 것입니다.

[8] 다음은 문제 해결을 위한 순서도입니다. 저희 프로그램에서는 기존에 준비해둔 데이터셋인 COCO를 모델에 학습시키는 것 외에도 사용자의 데이터를 입력받고 추가로 학습시켜 정확도를 높이는 방안을 고안하였습니다. 프로그램을 처음 샐행시켰을 때 사용자에게 바른 자세로 앉을 것을 요구하고 그 모습을 저장하여 가중치를 준 데이터로 학습 데이터셋에 포함시키는 것입니다. 합성곱 신경망을 통해 머신러닝 모델을 구성합니다. 합성곱 신경망은 시각적 자료를 분석할 때 사용되는 인공신경망입니다. 이렇게 만든 모델에 실시간 영상을 입력하여 자세 추정으로 신체의 관절을 추출한 후, 목과 척추가 기울어진 각도를 계산합니다. 계산된 각도를 기반으로 사용자의 자세를 점수로 환산합니다. 점수가 일정 수준 이하면 바르지 않은 자세로 판단하여 경고를 출력합니다.

[9] 이 프로그램을 구현하는 데에 필요한 기술 스택들을 소개하겠습니다. 먼저, 컴퓨터 비전 오픈소스 라이브러리로 opencv와 openpose, tensorflow가 있습니다. 
opencv는 Open Source Computer Vision의 약자로 오픈소스 컴퓨터 비전 라이즈러리 중 하나입니다. Window, Linux 등 다양한 플랫폼을 지원하고 C++, Python, Jave 등 다양한 언어를 지원합니다. 4.5.0부터 아파치 라이선스 2.0으로 배포되고 있으며 방대한 기능으로 컴퓨터 비전에서 필수적으로 사용하는 라이브러리라고 합니다. 
Openpose는 자세 추정 오픈소스입니다. Openpose는 별도의 센서나 장비가 필요없고 오로지 영상만을 가지고 딥러닝 합성곱 신경망을 기반하여 사람의 특징점을 추출합니다. 특징점은 관절이고 이후 추정한 관절들을 이어 사람의 골격을 찾습니다.  
Tendorflow는 쉽게 머신러닝 모델을 빌드할 수 있게 해주는 플랫폼입니다.
핵심 기능 코드는 파이썬으로 작성할 것입니다.
지금까지 자세 교정을 위한 진단 프로그램 연구가 다양하게 이루어지고 있지만 실제로 사용자들이 쉽게 접근할 수 있을 정도로 배포된 결과물은 없습니다. 따라서 이번 프로젝트에서는 프로그램을 사용자가 쉽게 이용할 수 있게 웹 기반의 어플리케이션으로 구현하는 것이 목표입니다. javascript 기반 프레임워크인 node.js로 프런트와 백엔드를 모두 구현할 수 있습니다. 

[10] 저희 프로그램의 테스트 시연 모습을 보여드리겠습니다. vs code에서 추가적인 코드를 작성하고 openpose 모델을 실행시켜 이러한 결과를 얻을 수 있었습니다. 아직 저희의 실력이 부족하여 기획에 가까운 프로그램을 만들어내지는 못했지만, 나중에라도 구현할 수 있도록 문서화하는 것에 목표를 두고 구상을 하였습니다. 왼쪽 사진에는 모델이 특징점을 추출한 모습을 볼 수 있고, 각 특징점들을 연결한 모습을 오른쪽 사진에서 볼 수 있습니다. 
사진으로부터 특징점을 추출하고, 그 결과를 이용해 사용자의 자세 판단을 어떻게 할 지에 대해 다양하게 생각을 해 보았습니다. 먼저, 특징점들을 이은 선분들 간의 각도를 계산하는 방법입니다. 각도 계산은 벡터를 활용하였습니다. 코인 0번 점과 목인 1번 점을 이은 선분과 목인 1번 점과 오른쪽 힙인 8번 점을 이은 각도를 angle1이라고 하였습니다. 이 사진에서는 angle1이 예각 기준으로 약 57도가 나왔습니다. angle2는 0과 1은 이은 선분과 1과 왼쪽 힙인 9번을 이은 선분 사이의 각도입니다.  이 테스트에서 angle2는 약 83도가 나왔습니다. 이 차이를 생각해 보았을 때, 3차원 인간의 모습을 2차원으로 투영시킨 사진으로 정확한 각도를 구하는 데에 한계가 있다는 것을 알 수 있었습니다.
두번째 고안한 방법은 목 특징점에서 x축에 수직인 직선을 긋고, 그 직선과 코-목 선분 사이의 각도를 구하는 것입니다. 이때 테스트 이미지에서 약 49도가 나왔습니다.
마지막으로 생각한 방법은 코-목 선분의 길이를 측정하는 것입니다. 사용자가 바른 자세로 앉아있을 때의 코-목 선분의 길이보다 사용자가 거북목 상태일 때의 코-목 선분의 길이가 더 길어질 것이라는 예측에서 도출한 방안입니다. 
세가지 방안에 각각 한계점들이 있을 것입니다. 이후 추가적인 구현을 할 때 팀원들과 더 정확한 측정을 할 수 있는 방안에 대해 토의할 예정입니다.
(교수님께서 문서화만 하면 구현은 어떻게든 된다고 하셨습니다.저희가 해낼 수 없는 영역이라면 프로그래머들에게 외주를 맡기면 됩니다.)

[11] 마지막으로 기획하고 발표 자료를 구성하며 참고한 자료들입니다. 이상으로 발표를 마치겠습니다. 경청해주셔서 감사합니다.